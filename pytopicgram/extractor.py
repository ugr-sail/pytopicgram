"""
This module is designed for extracting and processing text data from CSV files and training a BERTopic model on the extracted data. It provides functionality to read messages from a specified column in a CSV file, train a BERTopic model using these messages, and handle various data and model-related operations.

Functions:
- read_messages_from_csv: Reads messages from a specified column in a CSV file.
  - Parameters:
    - file_path (str): The path to the CSV file.
    - column (str): The name of the column from which to read the messages. Defaults to 'text'.
  - Returns:
    - list: A list containing the messages from the specified column.

- train_bertopic_model: Trains a BERTopic model on the provided list of messages.
  - Parameters:
    - messages (list): The list of messages to be used for training the model.
    - nr_topics (int): The number of topics to be generated by the model. Defaults to 100.
  - Returns:
    - tuple: A tuple containing the trained BERTopic model, the topics, and the probabilities.

Dependencies:
- csv: Used for reading CSV files.
- pandas: Used for data manipulation and analysis.
- bertopic: Used for topic modeling with BERT.
- argparse: Used for parsing command line arguments.
- sys: Used for system-specific parameters and functions.
"""

import pandas as pd
import numpy as np
import argparse
from rich import print
from bertopic import BERTopic
from bertopic.representation import KeyBERTInspired
from umap import UMAP
from sklearn.feature_extraction.text import CountVectorizer
import nltk
from nltk.corpus import stopwords
from sentence_transformers import SentenceTransformer
import openai
import tiktoken
from bertopic.representation import OpenAI
from bertopic.representation import PartOfSpeech
from bertopic.representation import MaximalMarginalRelevance

# --------------------------------------------------------------------------------
def read_messages_from_csv(file_path: str, column: str = 'message_nlp') -> list:
    """
    Reads messages from a specified column in a CSV file.

    Parameters:
    - file_path (str): The path to the CSV file.
    - column (str): The name of the column from which to read the messages. Defaults to 'message_nlp'.

    Returns:
    - list: A list containing the messages from the specified column.

    Raises:
    - ValueError: If the specified column does not exist in the CSV file.
    """
    df = pd.read_csv(file_path, encoding='utf-8')
    if column in df.columns:
        messages = df[column].tolist()
    else:
        raise ValueError(f"The specified column '{column}' does not exist in the CSV file.")
    return messages

# --------------------------------------------------------------------------------
def _configure_representation_model_OpenAI(api_key: str, nr_docs: int, model: str = "gpt-4o-mini"):
    # Tokenizer
    tokenizer= tiktoken.encoding_for_model(model)

    # Create your representation model
    client = openai.OpenAI(api_key=api_key)
    representation_model = OpenAI(
        client,
        model=model, 
        delay_in_seconds=2, 
        chat=True,
        nr_docs=nr_docs, 
        doc_length=100,
        tokenizer=tokenizer
    )

    # Change prompt to adapt to Spanish
    # representation_model.default_prompt_ = representation_model.default_prompt_ + "\nIMPORTANT: Please note that the documents will be in Spanish and your answer must be in Spanish."

    # Change prompt to adapt to the language used in the documents
    representation_model.default_prompt_ = representation_model.default_prompt_ + "\nIMPORTANT: Please note that all the provided documents are in the same language. Also, the description must be in the same language of the provided documents."

    return representation_model

# --------------------------------------------------------------------------------
def train_bertopic_model(messages: list, nr_topics: int = 100, nr_docs: int = 10, openai_key: str = None, sample_ratio: float = None, stopwords_lang: str = 'spanish'):
    """
    Trains a BERTopic model on the provided messages.

    Parameters:
    - messages (list): A list of messages to train the model on.
    - nr_topics (int): The number of topics to model. Defaults to 100, 0 to let BERTopic decide.
    - nr_docs (int): The number of documents to use for OpenAI representation. Defaults to 10.
    - openai_key (str, optional): OpenAI API key for using OpenAI representation model. Defaults to None.
    - sample_ratio (float, optional): Ratio of messages to sample for training. If None or not in (0, 1), use all messages.
    - stopwords_lang (str): The language of stopwords to use. Defaults to 'spanish'.
    
    Returns:
    - tuple: A tuple containing:
        - BERTopic: The trained BERTopic model.
        - list: The messages used for training (may be a sample of the input messages).
    """

    # Topic representations to be computed
    representation_model = {
        "Representation KeyBERT":    KeyBERTInspired(),
        "Representation POS":        PartOfSpeech("es_core_news_sm"),
        "Representation KeyBERTv2":  [KeyBERTInspired(top_n_words=30), MaximalMarginalRelevance(diversity=.5)]
    }

    if openai_key is not None:
        aspect_model3 = _configure_representation_model_OpenAI(openai_key, nr_docs)
        representation_model["Representation OpenAI"] = aspect_model3

    # Embeddings processing
    umap_model = UMAP(low_memory=True)
    nltk.download('stopwords')
    vectorizer_model = CountVectorizer(stop_words=stopwords.words(stopwords_lang))

    # Select sample of messages
    if sample_ratio:
        if sample_ratio > 0 and sample_ratio < 1:
            sample_size = int(sample_ratio * len(messages))
            print(f"Sampling {sample_size} messages ({sample_ratio * 100:.2f}% of total)")
            messages = pd.DataFrame(messages).sample(n=sample_size).iloc[:, 0].tolist()

    # Calculate topics
    if nr_topics > 0:
        topic_model = BERTopic(representation_model=representation_model,
                               vectorizer_model=vectorizer_model,
                               umap_model=umap_model,
                               language="spanish",
                               nr_topics=nr_topics,
                               verbose=True)
    else:
        topic_model = BERTopic(representation_model=representation_model,
                               vectorizer_model=vectorizer_model,
                               language="spanish",
                               verbose=True)

    topic_model.fit(documents=messages)  # , embeddings=embeddings)
    return topic_model, messages

# --------------------------------------------------------------------------------
# --------------------------------------------------------------------------------

def main():        
    # Create the parser
    parser = argparse.ArgumentParser(description="Train a BERTopic model from the input messages.")

    # Add the arguments
    parser.add_argument('-f', '--file', type=str, required=True, help="The path to the CSV file containing the messages.")
    parser.add_argument('-o', '--output', type=str, required=True, help="The output file for the model.")
    parser.add_argument('-c', '--column', type=str, required=False, default="message_nlp", help="Column name for the message once processed and nlp-treated.")
    parser.add_argument('-n', '--num_topics', type=int, required=False, default=100, help="Number of topics to generate. Defaults to 100.")
    parser.add_argument('-k', '--openai_key', type=str, required=False, help="If an OpenAI key is provided, a description with OpenAI is generated for the topics.")
    parser.add_argument('-nd', '--n_docs_openai', type=int, required=False, default=10, help="If an OpenAI key is provided, the number of documents used to find the description.")
    parser.add_argument('-r',  '--sample_ratio', type=float, required=False, help="Builds the topic model only with a sample of messages with size sample_ratio * 100.")
    parser.add_argument('-m',  '--messages_used', type=str, required=False, default="messages_used_for_model.csv", help="The path of the CSV where the messages used to train the model will be stored.")

    # Parse the arguments
    args = parser.parse_args()

    csv_file_path = args.file
    output_file_path = args.output
    text_column = args.column
    nr_topics = args.num_topics
    openai_key = args.openai_key
    n_docs_openai = args.n_docs_openai
    sample_ratio = args.sample_ratio
    messages_used_file_path = args.messages_used
    
    # Get messages from input file
    print(f"Reading messages from: {csv_file_path} ", end="")
    messages = read_messages_from_csv(csv_file_path, column=text_column)
    print(f"[[green]OK[/]] ({len(messages)} messages from the column '{text_column}').")
    
    # Train BERTopic model
    print(f"Creating BERTopic model from messages", end = "")
    topic_model, messages_used = train_bertopic_model(messages, nr_topics=nr_topics, nr_docs=n_docs_openai, openai_key=openai_key, sample_ratio=sample_ratio)
    num_topics_generated = len(topic_model.get_topic_info()) - 1  # Subtracting 1 to exclude the -1 (outlier) topic
    print(f"[[green]OK[/]] ({num_topics_generated} topics).")
    
    # Save messages actually used to generate the model
    print(f"Saving messages used to train the model ", end = "")
    messages_used_df = pd.DataFrame({text_column: messages_used})
    messages_used_df.to_csv(messages_used_file_path, index=False, encoding='utf-8')
    num_messages_used = len(messages_used)
    print(f"[[green]OK[/]] ({num_messages_used} messages).")

    # Save BERTopic model
    print(f"Writing BERTopic model to: {output_file_path} ", end = "")
    topic_model.representation_model = None  # Zeroing representation model, as per https://github.com/MaartenGr/BERTopic/issues/1658
    topic_model.save(output_file_path)
    print(f"[[green]OK[/]]")

if __name__ == "__main__":
    main()
